.. title: Seminar Series
.. slug: seminars
.. type: text

The RSE group organises seminars and promotes seminars from other groups which are of interest to the community. If you have an interesting seminar then announce it on the [mailing-list](../). Below are a number of seminars which are scheduled or have taken place.

This page provides a list of upcoming and previous seminars held by RSE@Sheffield group. We aim to hold lunchtime seminars on the last Tuesday of every month, inviting speakers to talk about a wide range of topics that involves practical issues relating to research software.

If you would like to recommend a speaker or would like to give a talk then please [contact us](../../contact). We have a budget to support inviting speakers.

# Upcoming Senimars #


### September 12, 2018 &mdash; 15:30-16:30, Hicks building, Lecture Theatre 7
<strong>Rob Baxter (EPCC)</strong><br/><br/>
**Talk:** *"Don’t Panic! Demystifying Big Data, Data Science and all that."*
<br/>
**Abstract:** The world’s digital data are still doubling in number every year or so, and research is a big culprit. Whether you measure it by volume, velocity or variety, research data is getting Big. All is not lost, though. It is challenging, and it does take a slightly different approach, but dealing with oodles of research data is not as terrifying as it seems. In this talk I aim to give researchers some practical ideas on working with “big” research data, illustrated with examples of how we manage stuff like this at EPCC. 
<br/>

# Previous Seminars #
### June 27, 2018 &mdash; 15:30-16:30, Hicks building, Lecture Theatre 7
<strong>Jos Martin (Mathworks)</strong><br/><br/>
**Talk:** *"Software engineering in practise"*
<br/>
**Abstract:** Jos will discuss how MathWorks develop and extend MATLAB and its associated tools from an engineering perspective. Focusing on the opportunities – and challenges – in ensuring MATLAB continues to be the tool of choice in the world of multicore CPUs, in grids and clouds, on mobile platforms, and on GPUs. He discusses the processes involved in developing and testing MATLAB to drive usability features, development environment enhancements, and overall reliability.

<br/>

**Bio:** Jos Martin is the senior engineering manager for parallel computing products at MathWorks. He has responsibility for all parts of Parallel Computing Toolbox, including the use of GPU and big data types in other areas. In addition, he is also responsible for MATLAB Drive, MATLAB Connector, and products that help link the desktop to the cloud. Before moving to a development role within MathWorks, he worked as a consultant in the U.K., writing large-scale MATLAB applications, particularly in the finance and automotive sectors. Prior to joining MathWorks in 2000, he held a Royal Society Post-Doctoral Fellowship at the University of Otago, New Zealand. His area of research was Experimental Bose-Einstein Condensation (BEC), a branch of low-temperature atomic physics.
<br/><br/>
<hr/>

### June 19, 2018 &mdash; 12:00-13:00, COM-G12-Main Lewin ###
<strong>Dr Patricio Ortiz, Newcastle University</strong><br/><br/>
**Talk:** *"Tackling the learning curve of scientific programming"*
<br/>
**Abstract:** Programming is part of the curriculum of students of computer science, and it will be complemented with other related subjects to make them knowledgeable on the subject. The situation of a science or engineering student is the opposite; typically they have one course to learn one language, and that language is usually not the one they will first face in real-life situations. This situation has occurred for decades, and it is likely not going to change, but there is a real need to better prepare science and engineering students to face the very steep learning curve of having to start programming as part of an ongoing project or their thesis.
Universities like ours offer excellent facilities like the HPCs supplied by CICS, yet the reality is that many students and young researchers may have never used a Unix based system, let alone a parallel system.

The book I wrote, "first steps in scientific programmings" aims at facilitating the passage through the learning curve by providing tips based on years of experience and my interaction with students and brilliant young researchers who did not have the opportunity to learn anywhere else the challenges which programming in a scientific environment involve.

I will briefly describe the points which I think are more important to emphasise, points which I've confirmed as important by interacting with other experienced researchers at the U. of Sheffield, who are trying to provide support for the people starting in this field.

Link for the book:
[https://sites.google.com/view/fsscientificprogramming/home](https://sites.google.com/view/fsscientificprogramming/home)

A supportive link:

[https://sites.google.com/a/sheffield.ac.uk/rcg/my-blog/research-computing-notes/firststepsinscientificprogramming](https://sites.google.com/a/sheffield.ac.uk/rcg/my-blog/research-computing-notes/firststepsinscientificprogramming)

<br/>

**Bio:** Patricio Ortiz holds a PhD. in astronomy from the University of Toronto, Canada. He has a keen interest in programming as a mean to create tools to help his research when no tools were available. He has taught at the graduate and undergraduate levels in subjects about astronomy, instrumentation, and applied programming. Throughout his career, Patricio has interacted with students at any level as well as post-graduates, helping him identify the most critical subjects needed by young scientists in the physical sciences and usually not covered by current literature. He has worked on projects involving automated super-nova detection systems; detection of fast moving solar system bodies, including Near-Earth objects and he was involved in the Gaia project (European Space Agency) for nearly ten years. Patricio also developed an ontology system used since its conception by the astronomical community to identify equivalent quantities. He also worked on an Earth Observation project, which gave him the opportunity to work extensively with high-performance computers, leading to his development of an automated task submission system which significantly decreases the execution time of data reduction of extended missions.


Patricio now works as a Research Software Engineer at the Department of Automatic Control and Systems Engineering at the University of Sheffield. He uses C, Fortran, Python, Java and Perl as his main toolkits, and as a pragmatic person, he uses the language which suits a problem best. Amongst his interests are: scientific data visualisation as a discovery tool, photography and (human) languages.
<br/>

<center><img src="/seminar_files/img/2018_06_19_curve.jpg" alt=""  style="max-width: 500px"></center>
<strong><a href="/seminar_files/slides/2018_06_19_curve_talk">SLIDES</a></strong>

<br/><br/>
<hr/>

### April 30, 2018 &mdash; 15:00-16:00, Broad Lane Lecture Theater 11 (BROAD-LT11) ###
<strong>Peter Heywood, University of Sheffield</strong><br/><br/>
**Talk:** *"Accelerating Road Network Simulations using GPUs"*
<br/>
**Abstract:** Road network Simulations are a vital tool used in the planning and management of transport network infrastructure. Large-scale simulations are computationally expensive tasks, taking many-hours for a single simulation to complete using multi-core CPU architectures, hindering the use of large-scale simulations.<br/>Using Graphics Processing Units (GPUs) we demonstrate that both Macroscopic (top-down) simulations and Microscopic (bottom-up) simulations can be considerably accelerated using many-core architectures and novel fine-grained data-parallel algorithms.<br/> A multi-GPU version of the SATURN macroscopic road network simulation package has been developed, demonstrating assignment performance improvements of over 11x compared to a multi-socket CPU. A FLAME GPU based microscopic simulation demonstrates performance improvements of up to 65x using a single GPU compared to the commercial multi-core microsimulation tool Aimsun.
<br/>

**Bio:** Peter Heywood is a Research Software Engineer and PhD candidate at the University of Sheffield. His research is focussed on using Graphics Processing Units (GPUs) to improve the performance of complex systems simulations; including transport network simulation and biological cellular simulations. Peter has presented his work at multiple international conferences and recently published his work in the Simulation Modelling Practice and Theory Journal.
<br/>

<center><img src="/seminar_files/img/2018_04_30_roadNetwork.jpg" alt=""  style="max-width: 500px"></center>

<br/><br/>
<hr/>

### April 16, 2018 &mdash; 15:30-16:30, Hicks building, Lecture Theatre 6
<strong>David Hubber, Researcher at Ludwig-Maximilians-Universität Munich</strong><br/><br/>
**Talk:** *"Structuring code efficiently"*
<br/>
**Abstract:** David will discuss how to structure code efficiently. He will also discuss code module design, decoupling strategies and test-driven development. 

<br/>

**Bio:** David Hubber is an astrophysicist by training that has worked extensively on writing software such as GANDALF and SEREN to simulate star forming regions. He has been a researcher at the university of Cardiff, the university of Sheffield, and is currently a Postdoc researcher Ludwig-Maximilians-Universitat Munich.
<br/><br/>
<hr/>

### November 28, 2017 &mdash; 11:00-12:00, Lecture Theatre G02, Firth Court ###
<strong>Dr Stephen McGough, Newcastle University</strong><br/><br/>
**Talk:** *"PARALLEM: massively Parallel Landscape Evolution Modelling"*
<br/>
**Abstract:** Landscape Evolution Modelling (LEM) is used to predict how landscapes evolve over the millennia due to weathering and erosion. LEM's normally operate on a regular grid of cells each representing the height of a point within a landscape. The process can be broken up into the following stages applied to each cell: determination of the flow direction of water out of that cell, summation of the volume of water flowing through the cell and computation of the erosion / deposition in the cell. This process is repeated at regular intervals - either an annual time-step (due to computation complexity) or once for each major storm-event - for periods of around one million years. <br/> Within each year/rainstorm there is great potential for speedup when executing on a GPGPU - flow direction for most cells can be performed independently, flow accumulation can easily be performed in parallel. However, due to certain 'real world' landscape features this potential can easily be lost. Landscape features such as plateaus - where all cells in a region have the same height - or sinks - (sets of) cells which have no lower neighbour, meaning water cannot escape - prevent optimal speedup being achieved. Likewise, computing the volume of water passing through a cell is inherently sequential - the sum of water through a cell at the end of a river depends on knowing the volume of water passing through all points upstream.<br/> CUDA algorithms have been developed for computing the LEM processes: Flow direction can be computed independently for each cell; A parallel breadth-first algorithm can be used for routing water over a plateau to an outlet cell; A parallel technique for 'filling' sinks (making them into lakes) which performs much of its work through parallel pointer jumping is used. Flow accumulation can be performed using a developed 'correct' algorithm where each cell which has no incorrect flows entering it can be computed and marked correct. The process of erosion / deposition can then be performed in a similar manner to flow accumulation with the eroded material being passed around. <br/> In this talk we will present parallel techniques for overcoming these problems, demonstrating between two and three orders of magnitude speedup over the best-in- class LEM software for landscapes between 0.1 and 56 million cells - far larger than the traditional 5 thousand cell simulations which have previously been performed. This has led to a need for a re-evaluation of the models used within the LEM community. Errors in LEM simulation results, which have been used over the last 30+ years, have been attributed to the very small simulation sizes. However, with our 46+ million cell simulations - a realistic scale - we are now able to determine that these errors are not due to scale but rather due to the equations themselves.<br/> We will present our approach moving forwards to overcome these limitations and present initial results of this work.
<br/>

**Bio:** Dr Stephen McGough is a Senior Lecturer in the School of Computing at Newcastle University, UK. He obtained his PhD in the area of Parallel simulation and has worked for many years in the areas of parallel computing and simulation. Holding posts at Imperial College London, UCL, Newcastle University and Durham University. This has led to over fifty publications in the area of parallel computing including receiving the NVIDIA best paper award at HiPC 2012. His research focuses on the use of novel computing technologies, such as GPGPU, to solve real-world challenges.
<br/>

<center><img src="/seminar_files/img/2017_11_28_parallem.jpg" alt=""  style="max-width: 500px"></center>
<strong><a href="/seminar_files/slides/2017_11_28_gpu_parallem_talk.pdf">SLIDES</a></strong>

<br/><br/>
<hr/>

### September 09, 2017 &mdash; 13:00-14:00, Workroom 3 (205), The Diamond ###
<strong>Dr Thomas Nowotny, University of Sussex</strong><br/><br/>
**Talk:** *"Opportunities and challenges for spiking neural networks on GPUs"*
<br/>
**Abstract:** In the past 6 years, we have developed the GeNN (GPU enhanced neuronal networks) framework for GPU accelerated spiking neuronal network simulations. In essence, GeNN is based on a simple design of code generation that allows a large extent of flexibility for computational models while at the same time taking care of some of the GPU specific optimisation work in the background. In this talk I will present the main features of GeNN, its design principles and show benchmarks. I will then discuss limitations, both specific to GeNN and to numerical simulation work on GPUs more generally and present some further work, including the SpineML-GeNN and Brian2GeNN interfaces. GeNN is developed within the Green Brain [http://greenbrain.group.shef.ac.uk/](http://greenbrain.group.shef.ac.uk/) and Brains on Board [http://brainsonboard.co.uk/](http://brainsonboard.co.uk/) projects and is available under GPL v2 at Github [http://genn-team.github.io/genn/](http://genn-team.github.io/genn/).
<br/>

**Bio:** Thomas Nowotny received his PhD in theoretical physics in 2001 from the University of Leipzig and worked for five years at the University of California, San Diego. In 2007 he joined the University of Sussex, where he is now a Professor of Informatics and the Director for Research and Knowledge Exchange at the School of Engineering and Informatics. His research interests include olfaction in animals and machines, GPU accelerated scientific computing, hybrid brain-computer systems and bio-inspired machine learning.
<br/>

<center><img src="/seminar_files/img/2017_09_27_thomas_nowotny.jpg" alt=""  style="max-width: 500px"></center>
<strong><a href="https://sussex.box.com/s/8i7dby89e6fj2t8lw447y68dkrsc1oo5">SLIDES</a></strong>

<br/><br/>
<hr/>

### August 08, 2017 &mdash; 13:00-14:00, Workroom 3 (205), The Diamond ###
<strong>Twin Karmakharm, Research Software Engineer, University of Sheffield</strong><br/><br/>
**Talk:** *"Containers for High Performance Computing"*
<br/>
**Abstract:** Containerization is a lightweight virtualisation technology where users can package workflows, software, libraries and data for running on various machines with minimal loss of performance. The technology can be used a way for scientists to deploy custom software on the HPC cluster and easily share reproducible software along with their data. The talk explores the concept of containers and existing container technologies suitable for the HPC. Particular focus is given for Singularity which has recently been introduced on the new Sheffield Advance Research Computer (ShARC) cluster.
<br/>

<center><img src="/seminar_files/img/2017_08_29_gpu_containers.jpg" alt=""  style="max-width: 500px"></center>


<br/><br/>
<hr/>

### May 30, 2017 &mdash; 13:00-14:00, Workroom 2 (G05), The Diamond ###
<strong>Ania Brown, Research Software Engineer, Oxford e-Research Centre</strong><br/><br/>
**Talk:** *"Towards achieving GPU-native adaptive mesh refinement"*
<br/>
**Abstract:** Modern simulations model increasingly complex multiscale systems, and the need to capture details at multiple length scales can lead to large memory requirements. Adaptive mesh refinement (AMR) is a method for reducing memory cost by varying the accuracy in each region to match the physical characteristics of the simulation, at the cost of increased data structure complexity. This complexity is a particular problem on the GPU architecture, which is most naturally suited to regular data sets. I will describe some of the optimisation and software challenges that need to be considered when implementing AMR on GPUs, based on my experience working on a GPU-native framework for stencil calculations on a tree-based adaptively refined mesh as part of my Master degree. Topics covered will include achieving coalesced access with the AMR data structure, memory defragmentation after grid changes and load balancing using space-filling curves.
<br/>

**Bio:** Ania is a research software engineer at the Oxford e-Research Centre. Her research interests are a combination of performance optimisation for large scale scientific simulation and software development methodology to improve the quality of such codes. She received her Master degree from the Tokyo Institute of Technology in 2015.
<br/>

<center><img src="/seminar_files/img/2017_05_30_gpu_amr.jpg" alt=""  style="max-width: 400px"></center>
<strong><a href="/seminar_files/slides/2017_05_30_gpu_amr_talk.pdf">SLIDES</a></strong>


<br/><br/>

*Note: The [GPU computing](http://gpucomputing.shef.ac.uk/seminars/) seminar series has now been merged with the RSE seminar series. Talks on GPU computing will be advertised through both lists.*
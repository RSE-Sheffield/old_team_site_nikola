.. title: Seminar Series
.. slug: seminars
.. type: text

The RSE group organises seminars and promotes seminars from other groups which are of interest to the community. If you have an interesting seminar then announce it on the [mailing-list](../). Below are a number of seminars which are scheduled or have taken place.

<!--
* 12th September 2018 - "Demystifying big data, machine learning and all that [[more info](https://www.sheffield.ac.uk/physics/news/programming-skills-sessions)]" by Rob Baxter (EPCC)
* 27th June 2018 - "Software engineering in practise [announcement](https://groups.google.com/a/sheffield.ac.uk/forum/#!topic/rse-group/1CumYPMHsXg)" by Joz Martin (Mathworks)
* 19th June 2018 - "[Tackling the learning curve of scientific programming](../../blog/2018-06-19-rse-computing-seminar-coffe-cake)" by Patrizo Ortiz
* 30th April 2018 - "Accelerating Road Network Simulations using GPUs [[pdf](http://ptheywood.uk/download/presentations/heywood-com4521-2018.pdf)]" by Peter Heywood (RSE team member)
* 16th April 2018 - "Structuring code efficiently [[announcement](https://groups.google.com/a/sheffield.ac.uk/forum/#!topic/rse-group/lWAmtjW2d4o)]" by David Hubber ((Ludwig-Maximilians-University Munich))

Talks prior to 2018 can be found by looking through the [mailing-list](../)-->


This page provides a list of upcoming and previous seminars held by RSE@Sheffield group. We aim to hold lunchtime seminars on the last Tuesday of every month, inviting speakers to talk about a wide range of topics that involves practical issues relating to research software.

If you would like to recommend a speaker or would like to give a talk then please [contact us](../../contact). We have a budget to support inviting speakers.

<!--If you have an idea for a talk or would like to invite someone to give an interesting talk then please [contact us](../../contact). We have a budget to support inviting speakers.-->



# Upcoming Senimars #

<!--<strong>No offical talks are held by RSE@Sheffield in September.</strong><br/><br/>-->

### September 12, 2018 &mdash; 15:30-16:30, Hicks building, lecture theatre 7
<strong>Rob Baxter (EPCC)</strong><br/><br/>
**Talk:** *"Demystifying big data, machine learning and all that"*
<br/>
**Abstract:** [[more info](https://www.sheffield.ac.uk/physics/news/programming-skills-sessions)]" 

<br/>

# Previous Seminars #
### June 27, 2018 &mdash; 15:30-16:30, Hicks building, lecture theatre 7
<strong>Joz Martin (Mathworks)</strong><br/><br/>
**Talk:** *"Software engineering in practise"*
<br/>
**Abstract:**  [announcement](https://groups.google.com/a/sheffield.ac.uk/forum/#!topic/rse-group/1CumYPMHsXg)

<br/>

### June 19, 2018 &mdash; 12:00-13:00, COM-G12-Main Lewin ###
<strong>Dr Patricio Ortiz, Newcastle University <a href="/seminar_files/bio/patricio_ortiz.md" target="popup" onclick="window.open('/seminar_files/bio/patricio_ortiz.md','name','width=600,height=400')">(Bio)</a></strong><br/><br/>
**Talk:** *"Tackling the learning curve of scientific programming"*
<br/>
**Abstract:** Programming is part of the curriculum of students of computer science, and it will be complemented with other related subjects to make them knowledgeable on the subject. The situation of a science or engineering student is the opposite; typically they have one course to learn one language, and that language is usually not the one they will first face in real-life situations. This situation has occurred for decades, and it is likely not going to change, but there is a real need to better prepare science and engineering students to face the very steep learning curve of having to start programming as part of an ongoing project or their thesis.
Universities like ours offer excellent facilities like the HPCs supplied by CICS, yet the reality is that many students and young researchers may have never used a Unix based system, let alone a parallel system.

The book I wrote, "first steps in scientific programmings" aims at facilitating the passage through the learning curve by providing tips based on years of experience and my interaction with students and brilliant young researchers who did not have the opportunity to learn anywhere else the challenges which programming in a scientific environment involve.

I will briefly describe the points which I think are more important to emphasise, points which I've confirmed as important by interacting with other experienced researchers at the U. of Sheffield, who are trying to provide support for the people starting in this field.

Link for the book:
[https://sites.google.com/view/fsscientificprogramming/home](https://sites.google.com/view/fsscientificprogramming/home)

A supportive link:

[https://sites.google.com/a/sheffield.ac.uk/rcg/my-blog/research-computing-notes/firststepsinscientificprogramming](https://sites.google.com/a/sheffield.ac.uk/rcg/my-blog/research-computing-notes/firststepsinscientificprogramming)

<br/>
<center><img src="/seminar_files/img/2018_06_19_curve.jpg" alt=""  style="max-width: 500px"></center>
<strong><a href="/seminar_files/slides/2018_06_19_curve_talk">SLIDES</a></strong>

<br/><br/>

### April 30, 2018 &mdash; 15:00-16:00, Broad Lane Lecture Theater 11 (BROAD-LT11) ###
<strong>Peter Heywood, University of Sheffield<a href="/seminar_files/bio/peter-heywood.md" target="popup" onclick="window.open('/seminar_files/bio/peter-heywood.md','name','width=600,height=400')"> (Bio)</a></strong><br/><br/>
**Talk:** *"Accelerating Road Network Simulations using GPUs"*
<br/>
**Abstract:** Road network Simulations are a vital tool used in the planning and management of transport network infrastructure. Large-scale simulations are computationally expensive tasks, taking many-hours for a single simulation to complete using multi-core CPU architectures, hindering the use of large-scale simulations.<br/>Using Graphics Processing Units (GPUs) we demonstrate that both Macroscopic (top-down) simulations and Microscopic (bottom-up) simulations can be considerably accelerated using many-core architectures and novel fine-grained data-parallel algorithms.<br/> A multi-GPU version of the SATURN macroscopic road network simulation package has been developed, demonstrating assignment performance improvements of over 11x compared to a multi-socket CPU. A FLAME GPU based microscopic simulation demonstrates performance improvements of up to 65x using a single GPU compared to the commercial multi-core microsimulation tool Aimsun.
<br/>
<center><img src="/seminar_files/img/2018_04_30_roadNetwork.jpg" alt=""  style="max-width: 500px"></center>

<br/><br/>

### April 16, 2018 &mdash; 15:30-16:30, Hicks building, lecture theatre 6
<strong>David Hubber, Researcher at Ludwig-Maximilians-Universit√§t Munich </strong><br/><br/>
**Talk:** *"Structuring code efficiently"*
<br/>
**Abstract:** [[announcement](https://groups.google.com/a/sheffield.ac.uk/forum/#!topic/rse-group/lWAmtjW2d4o)]

<br/>

### November 28, 2017 &mdash; 11:00-12:00, Lecture Theatre G02, Firth Court ###
<strong>Dr Stephen McGough, Newcastle University<a href="/seminar_files/bio/stephen-mcgough.md" target="popup" onclick="window.open('/seminar_files/bio/stephen-mcgough.md','name','width=600,height=400')"> (Bio)</a></strong><br/><br/>
**Talk:** *"PARALLEM: massively Parallel Landscape Evolution Modelling"*
<br/>
**Abstract:** Landscape Evolution Modelling (LEM) is used to predict how landscapes evolve over the millennia due to weathering and erosion. LEM's normally operate on a regular grid of cells each representing the height of a point within a landscape. The process can be broken up into the following stages applied to each cell: determination of the flow direction of water out of that cell, summation of the volume of water flowing through the cell and computation of the erosion / deposition in the cell. This process is repeated at regular intervals - either an annual time-step (due to computation complexity) or once for each major storm-event - for periods of around one million years. <br/> Within each year/rainstorm there is great potential for speedup when executing on a GPGPU - flow direction for most cells can be performed independently, flow accumulation can easily be performed in parallel. However, due to certain 'real world' landscape features this potential can easily be lost. Landscape features such as plateaus - where all cells in a region have the same height - or sinks - (sets of) cells which have no lower neighbour, meaning water cannot escape - prevent optimal speedup being achieved. Likewise, computing the volume of water passing through a cell is inherently sequential - the sum of water through a cell at the end of a river depends on knowing the volume of water passing through all points upstream.<br/> CUDA algorithms have been developed for computing the LEM processes: Flow direction can be computed independently for each cell; A parallel breadth-first algorithm can be used for routing water over a plateau to an outlet cell; A parallel technique for 'filling' sinks (making them into lakes) which performs much of its work through parallel pointer jumping is used. Flow accumulation can be performed using a developed 'correct' algorithm where each cell which has no incorrect flows entering it can be computed and marked correct. The process of erosion / deposition can then be performed in a similar manner to flow accumulation with the eroded material being passed around. <br/> In this talk we will present parallel techniques for overcoming these problems, demonstrating between two and three orders of magnitude speedup over the best-in- class LEM software for landscapes between 0.1 and 56 million cells - far larger than the traditional 5 thousand cell simulations which have previously been performed. This has led to a need for a re-evaluation of the models used within the LEM community. Errors in LEM simulation results, which have been used over the last 30+ years, have been attributed to the very small simulation sizes. However, with our 46+ million cell simulations - a realistic scale - we are now able to determine that these errors are not due to scale but rather due to the equations themselves.<br/> We will present our approach moving forwards to overcome these limitations and present initial results of this work.
<br/>
<center><img src="/seminar_files/img/2017_11_28_parallem.jpg" alt=""  style="max-width: 500px"></center>
<strong><a href="/seminar_files/slides/2017_11_28_gpu_parallem_talk.pdf">SLIDES</a></strong>

<br/><br/>
### September 09, 2017 &mdash; 13:00-14:00, Workroom 3 (205), The Diamond ###
<strong>Dr Thomas Nowotny, University of Sussex<a href="/seminar_files/bio/thomas_nowotny.md" target="popup" onclick="window.open('/seminar_files/bio/thomas_nowotny.md','name','width=600,height=400')"> (Bio)</a></strong><br/><br/>
**Talk:** *"Opportunities and challenges for spiking neural networks on GPUs"*
<br/>
**Abstract:** In the past 6 years, we have developed the GeNN (GPU enhanced neuronal networks) framework for GPU accelerated spiking neuronal network simulations. In essence, GeNN is based on a simple design of code generation that allows a large extent of flexibility for computational models while at the same time taking care of some of the GPU specific optimisation work in the background. In this talk I will present the main features of GeNN, its design principles and show benchmarks. I will then discuss limitations, both specific to GeNN and to numerical simulation work on GPUs more generally and present some further work, including the SpineML-GeNN and Brian2GeNN interfaces. GeNN is developed within the Green Brain [http://greenbrain.group.shef.ac.uk/](http://greenbrain.group.shef.ac.uk/) and Brains on Board [http://brainsonboard.co.uk/](http://brainsonboard.co.uk/) projects and is available under GPL v2 at Github [http://genn-team.github.io/genn/](http://genn-team.github.io/genn/).
<br/>
<center><img src="/seminar_files/img/2017_09_27_thomas_nowotny.jpg" alt=""  style="max-width: 500px"></center>
<strong><a href="https://sussex.box.com/s/8i7dby89e6fj2t8lw447y68dkrsc1oo5">SLIDES</a></strong>

<br/><br/>
### August 08, 2017 &mdash; 13:00-14:00, Workroom 3 (205), The Diamond ###
<strong>Twin Karmakharm, Research Software Engineer, University of Sheffield</strong><br/><br/>
**Talk:** *"Containers for High Performance Computing"*
<br/>
**Abstract:** Containerization is a lightweight virtualisation technology where users can package workflows, software, libraries and data for running on various machines with minimal loss of performance. The technology can be used a way for scientists to deploy custom software on the HPC cluster and easily share reproducible software along with their data. The talk explores the concept of containers and existing container technologies suitable for the HPC. Particular focus is given for Singularity which has recently been introduced on the new Sheffield Advance Research Computer (ShARC) cluster.
<br/>
<center><img src="/seminar_files/img/2017_08_29_gpu_containers.jpg" alt=""  style="max-width: 500px"></center>


<br/><br/>
### May 30, 2017 &mdash; 13:00-14:00, Workroom 2 (G05), The Diamond ###
<strong>Ania Brown, Research Software Engineer, Oxford e-Research Centre<a href="/seminar_files/bio/ania_brown.md" target="popup" onclick="window.open('/seminar_files/bio/ania_brown.md','name','width=600,height=400')"> (Bio)</a></strong><br/><br/>
**Talk:** *"Towards achieving GPU-native adaptive mesh refinement"*
<br/>
**Abstract:** Modern simulations model increasingly complex multiscale systems, and the need to capture details at multiple length scales can lead to large memory requirements. Adaptive mesh refinement (AMR) is a method for reducing memory cost by varying the accuracy in each region to match the physical characteristics of the simulation, at the cost of increased data structure complexity. This complexity is a particular problem on the GPU architecture, which is most naturally suited to regular data sets. I will describe some of the optimisation and software challenges that need to be considered when implementing AMR on GPUs, based on my experience working on a GPU-native framework for stencil calculations on a tree-based adaptively refined mesh as part of my Master degree. Topics covered will include achieving coalesced access with the AMR data structure, memory defragmentation after grid changes and load balancing using space-filling curves.
<br/>
<center><img src="/seminar_files/img/2017_05_30_gpu_amr.jpg" alt=""  style="max-width: 400px"></center>
<strong><a href="/seminar_files/slides/2017_05_30_gpu_amr_talk.pdf">SLIDES</a></strong>


<br/><br/>

*Note: The [GPU computing](http://gpucomputing.shef.ac.uk/seminars/) seminar series has now been merged with the RSE seminar series. Talks on GPU computing will be advertised through both lists.*